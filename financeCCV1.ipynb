{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from io import BytesIO\n",
    "from tensorflow.contrib import learn\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the input dataset \n",
    "d = pd.read_csv(\"./consumer_complaints.csv\", \n",
    "                usecols=('product','consumer_complaint_narrative'),\n",
    "                dtype={'consumer_complaint_narrative': object})\n",
    "# Only interested in data with consumer complaints\n",
    "d=d[d['consumer_complaint_narrative'].notnull()]\n",
    "d=d[d['product'].notnull()]\n",
    "d.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (66806, 2)\n",
      "           product                       consumer_complaint_narrative\n",
      "0  Debt collection  XXXX has claimed I owe them {$27.00} for XXXX ...\n",
      "1    Consumer Loan  Due to inconsistencies in the amount owed that...\n",
      "2         Mortgage  In XX/XX/XXXX my wages that I earned at my job...\n",
      "3         Mortgage  I have an open and current mortgage with Chase...\n",
      "4         Mortgage  XXXX was submitted XX/XX/XXXX. At the time I s...\n",
      "\n",
      "List of Products       Occurrences\n",
      "\n",
      "Debt collection            17552\n",
      "Mortgage                   14919\n",
      "Credit reporting           12526\n",
      "Credit card                 7929\n",
      "Bank account or service     5711\n",
      "Consumer Loan               3678\n",
      "Student loan                2128\n",
      "Prepaid card                 861\n",
      "Payday loan                  726\n",
      "Money transfers              666\n",
      "Other financial service      110\n",
      "Name: product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's see what's in the data \n",
    "print (\"Data dimensions:\", d.shape)\n",
    "print (d.head())\n",
    "\n",
    "# Let's see a table of how many examples we have of each product\n",
    "print (\"\\nList of Products       Occurrences\\n\")\n",
    "print (d[\"product\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning (partially modified)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9()!?\\'\\`%$]\", \" \", string) # keep also %$ but removed comma\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\$\", \" $ \", string) #yes, isolate $\n",
    "    string = re.sub(r\"\\%\", \" % \", string) #yes, isolate %\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # fixing XXX and xxx like as word\n",
    "    string = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",string)\n",
    "    # removing non ascii\n",
    "    string = re.sub(r'[^\\x00-\\x7F]+', \"\", string) \n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning time: mine = 41.8 s, here = 36.1 s\n"
     ]
    }
   ],
   "source": [
    "word_data=[]\n",
    "t0 = time()\n",
    "\n",
    "for message in d['consumer_complaint_narrative']:\n",
    "    word_data.append(clean_str(message))\n",
    "\n",
    "# With a MacBook Pro (Late 2011)\n",
    "# 2.4 GHz Intel Core i5, 4 GB 1333 MHz DDR3\n",
    "print (\"\\nCleaning time: mine = 41.8 s, here =\", round(time()-t0, 1), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx has claimed i owe them $ 27 00 for xxx years despite the proof of payment i sent them canceled check and their ownpaid invoice for $ 27 00 ! they continue to insist i owe them and collection agencies are after me how can i stop this harassment for a bill i already paid four years ago ?\n"
     ]
    }
   ],
   "source": [
    "print(word_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52942 unique tokens.\n",
      "Shape of data tensor: (66806, 80)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     1    43   760     3   213    29  2699    27    16     1   107   621\n",
      "     2   256     8    35     3    82    29  1328   124     5    55 26521\n",
      "  1645    16  2699    27    11   335     4  2421     3   213    29     5\n",
      "   108   352    36    66    18   145    77     3   304    17   984    16\n",
      "     6   162     3   296    83  1306   107   295]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 80\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(word_data)\n",
    "sequencestr1 = tokenizer.texts_to_sequences(word_data)\n",
    "\n",
    "#print(tokenizer)\n",
    "#print(sequences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "datatr1 = pad_sequences(sequencestr1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', datatr1.shape)\n",
    "print(datatr1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(52943, 100)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(r'C:\\Users\\HuaSheng\\Desktop\\reddragonai\\dl_dev_course-master\\redaicse\\LSTMProject', 'glove6B100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Debt collection': 0, 'Mortgage': 2, 'Consumer Loan': 1, 'Bank account or service': 6, 'Payday loan': 7, 'Credit reporting': 4, 'Money transfers': 8, 'Other financial service': 9, 'Credit card': 3, 'Student loan': 5, 'Prepaid card': 10}\n"
     ]
    }
   ],
   "source": [
    "tgtDict = {u:v for v,u in enumerate(d[\"product\"].unique())}\n",
    "print(tgtDict)\n",
    "trainLabel = np.array([ tgtDict[i] for i in d[\"product\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ..., 7 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print(pdtClass)\n",
    "trainLabel = to_categorical(trainLabel,11)\n",
    "trainLabel[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tr1Inp (InputLayer)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 80, 100)           5294300   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 80, 100)           60400     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 80, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 5,446,871\n",
      "Trainable params: 152,115\n",
      "Non-trainable params: 5,294,756\n",
      "_________________________________________________________________\n",
      "Train on 60125 samples, validate on 6681 samples\n",
      "Epoch 1/5\n",
      "60125/60125 [==============================] - 544s - loss: 1.3275 - acc: 0.5597 - val_loss: 1.0521 - val_acc: 0.6613\n",
      "Epoch 2/5\n",
      "37952/60125 [=================>............] - ETA: 226s - loss: 1.0086 - acc: 0.6752"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model,Model\n",
    "from keras.layers import *\n",
    "import keras\n",
    "\n",
    "biLSTM1 = Bidirectional(LSTM(50,return_sequences=True))\n",
    "biLSTM2 = Bidirectional(LSTM(50))\n",
    "dense1 =  Dense(128, activation='relu')\n",
    "dense2 =  Dense(128, activation='relu')\n",
    "def processBlk(sequenceInp):\n",
    "    embedded_sequences = embedding_layer(sequenceInp)\n",
    "    x = biLSTM1(embedded_sequences)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = biLSTM2(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = dense1(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = dense2(x)\n",
    "    return(x)\n",
    "\n",
    "sequence_inputtr1 = Input(shape=(MAX_SEQUENCE_LENGTH,),name = 'tr1Inp', dtype='int32')\n",
    "x1 = processBlk(sequence_inputtr1)\n",
    "\n",
    "\n",
    "preds = Dense(11, activation='softmax')(x1)\n",
    "\n",
    "\n",
    "model = Model(sequence_inputtr1, preds)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit(datatr1, trainLabel, validation_split=0.1,shuffle=True,\n",
    "          epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
