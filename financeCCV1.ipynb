{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from io import BytesIO\n",
    "from tensorflow.contrib import learn\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the input dataset \n",
    "d = pd.read_csv(\"./consumer_complaints.csv\", \n",
    "                usecols=('product','consumer_complaint_narrative'),\n",
    "                dtype={'consumer_complaint_narrative': object})\n",
    "# Only interested in data with consumer complaints\n",
    "d=d[d['consumer_complaint_narrative'].notnull()]\n",
    "d=d[d['product'].notnull()]\n",
    "d.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (66806, 2)\n",
      "           product                       consumer_complaint_narrative\n",
      "0  Debt collection  XXXX has claimed I owe them {$27.00} for XXXX ...\n",
      "1    Consumer Loan  Due to inconsistencies in the amount owed that...\n",
      "2         Mortgage  In XX/XX/XXXX my wages that I earned at my job...\n",
      "3         Mortgage  I have an open and current mortgage with Chase...\n",
      "4         Mortgage  XXXX was submitted XX/XX/XXXX. At the time I s...\n",
      "\n",
      "List of Products       Occurrences\n",
      "\n",
      "Debt collection            17552\n",
      "Mortgage                   14919\n",
      "Credit reporting           12526\n",
      "Credit card                 7929\n",
      "Bank account or service     5711\n",
      "Consumer Loan               3678\n",
      "Student loan                2128\n",
      "Prepaid card                 861\n",
      "Payday loan                  726\n",
      "Money transfers              666\n",
      "Other financial service      110\n",
      "Name: product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's see what's in the data \n",
    "print (\"Data dimensions:\", d.shape)\n",
    "print (d.head())\n",
    "\n",
    "# Let's see a table of how many examples we have of each product\n",
    "print (\"\\nList of Products       Occurrences\\n\")\n",
    "print (d[\"product\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning (partially modified)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9()!?\\'\\`%$]\", \" \", string) # keep also %$ but removed comma\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\$\", \" $ \", string) #yes, isolate $\n",
    "    string = re.sub(r\"\\%\", \" % \", string) #yes, isolate %\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # fixing XXX and xxx like as word\n",
    "    string = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",string)\n",
    "    # removing non ascii\n",
    "    string = re.sub(r'[^\\x00-\\x7F]+', \"\", string) \n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning time: mine = 41.8 s, here = 26.1 s\n"
     ]
    }
   ],
   "source": [
    "word_data=[]\n",
    "t0 = time()\n",
    "\n",
    "for message in d['consumer_complaint_narrative']:\n",
    "    word_data.append(clean_str(message))\n",
    "\n",
    "# With a MacBook Pro (Late 2011)\n",
    "# 2.4 GHz Intel Core i5, 4 GB 1333 MHz DDR3\n",
    "print (\"\\nCleaning time: mine = 41.8 s, here =\", round(time()-t0, 1), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx has claimed i owe them $ 27 00 for xxx years despite the proof of payment i sent them canceled check and their ownpaid invoice for $ 27 00 ! they continue to insist i owe them and collection agencies are after me how can i stop this harassment for a bill i already paid four years ago ?\n"
     ]
    }
   ],
   "source": [
    "print(word_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52942 unique tokens.\n",
      "Shape of data tensor: (66806, 120)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     1    43   760     3   213    29  2699    27\n",
      "    16     1   107   621     2   256     8    35     3    82    29  1328\n",
      "   124     5    55 26521  1645    16  2699    27    11   335     4  2421\n",
      "     3   213    29     5   108   352    36    66    18   145    77     3\n",
      "   304    17   984    16     6   162     3   296    83  1306   107   295]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 120\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(word_data)\n",
    "sequencestr1 = tokenizer.texts_to_sequences(word_data)\n",
    "\n",
    "#print(tokenizer)\n",
    "#print(sequences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "datatr1 = pad_sequences(sequencestr1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print('Shape of data tensor:', datatr1.shape)\n",
    "print(datatr1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(52943, 100)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(r'C:\\Users\\HuaSheng\\Desktop\\reddragonai\\dl_dev_course-master\\redaicse\\LSTMProject', 'glove6B100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Debt collection': 0, 'Consumer Loan': 1, 'Mortgage': 2, 'Credit card': 3, 'Credit reporting': 4, 'Student loan': 5, 'Bank account or service': 6, 'Payday loan': 7, 'Money transfers': 8, 'Other financial service': 9, 'Prepaid card': 10}\n"
     ]
    }
   ],
   "source": [
    "tgtDict = {u:v for v,u in enumerate(d[\"product\"].unique())}\n",
    "inv_map = {v:u for v,u in enumerate(d[\"product\"].unique())}\n",
    "print(tgtDict)\n",
    "pdtClass = np.array([ tgtDict[i] for i in d[\"product\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ..., 7 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print(pdtClass)\n",
    "trainLabel = to_categorical(pdtClass,11)\n",
    "trainLabel[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tr1Inp (InputLayer)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 120, 100)          5294300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 120, 100)          60400     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "clusterOut1 (Dense)          (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "clusterOut2 (Dense)          (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                2827      \n",
      "=================================================================\n",
      "Total params: 5,510,999\n",
      "Trainable params: 215,987\n",
      "Non-trainable params: 5,295,012\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model,Model\n",
    "from keras.layers import *\n",
    "import keras\n",
    "\n",
    "biLSTM1 = Bidirectional(LSTM(50,return_sequences=True))\n",
    "biLSTM2 = Bidirectional(LSTM(50))\n",
    "dense1 =  Dense(256, activation='relu',name='clusterOut1')\n",
    "dense2 =  Dense(256, activation='relu',name='clusterOut2')\n",
    "def processBlk(sequenceInp):\n",
    "    embedded_sequences = embedding_layer(sequenceInp)\n",
    "    x = biLSTM1(embedded_sequences)\n",
    "    x = biLSTM2(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = dense1(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = dense2(x)\n",
    "    return(x)\n",
    "\n",
    "sequence_inputtr1 = Input(shape=(MAX_SEQUENCE_LENGTH,),name = 'tr1Inp', dtype='int32')\n",
    "x1 = processBlk(sequence_inputtr1)\n",
    "\n",
    "\n",
    "preds = Dense(11, activation='softmax')(x1)\n",
    "\n",
    "\n",
    "model = Model(sequence_inputtr1, preds)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60125 samples, validate on 6681 samples\n",
      "Epoch 1/1\n",
      "60125/60125 [==============================] - 976s - loss: 0.6495 - acc: 0.7947 - val_loss: 0.7456 - val_acc: 0.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b2d5182898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(pdtClass), pdtClass)\n",
    "model.fit(datatr1, trainLabel, validation_split=0.1,shuffle=True, class_weight=class_weight,\n",
    "          epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = model.get_layer('clusterOut2').output\n",
    "encoder_model = Model(sequence_inputtr1 , [dataS ,preds])\n",
    "dataSpace=encoder_model.predict(datatr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./financeModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66806, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSpace[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 10000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 10000\n",
      "[t-SNE] Mean sigma: 2.348675\n",
      "[t-SNE] KL divergence after 100 iterations with early exaggeration: 1.446722\n",
      "[t-SNE] Error after 375 iterations: 1.446722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2,verbose=1).fit_transform(dataSpace[0][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\HuaSheng\\\\Desktop\\\\hdbHackerthon\\\\temp-plot.html'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "trace = Scatter(x = X_embedded[:,0],y = X_embedded[:,1],text = d[\"product\"][:10000],mode = 'markers',marker=dict(\n",
    "        size='16',\n",
    "        color = np.divide(pdtClass,11), #set color equal to a variable\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ),textfont=dict( size=8, color='#7f7f7f')\n",
    ")\n",
    "plot([trace])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HuaSheng\\AppData\\Local\\conda\\conda\\envs\\tfkrnv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning:\n",
      "\n",
      "Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv(\"./consumer_complaints.csv\", \n",
    "                dtype={'consumer_complaint_narrative': object})\n",
    "# Only interested in data with consumer complaints\n",
    "d=d[d['consumer_complaint_narrative'].notnull()]\n",
    "d=d[d['product'].notnull()]\n",
    "d.reset_index(drop=True,inplace=True)\n",
    "d =d[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Debt collection', 'Debt collection', 'Mortgage', 'Mortgage', 'Mortgage', 'Credit reporting', 'Mortgage', 'Mortgage', 'Mortgage', 'Consumer Loan']\n",
      "0    Debt collection\n",
      "1      Consumer Loan\n",
      "2           Mortgage\n",
      "3           Mortgage\n",
      "4           Mortgage\n",
      "5           Mortgage\n",
      "6           Mortgage\n",
      "7           Mortgage\n",
      "8        Credit card\n",
      "9      Consumer Loan\n",
      "Name: product, dtype: object\n"
     ]
    }
   ],
   "source": [
    "classifiedOut = [inv_map[i] for i in np.argmax(dataSpace[1][:10000],axis=1)]\n",
    "print(classifiedOut[:10])\n",
    "print(d['product'][:10])\n",
    "d['classifiedOut'] = classifiedOut\n",
    "d['featureSpace1']=dataSpace[0][:10000,0]\n",
    "d['featureSpace2']=dataSpace[0][:10000,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.to_csv('financeSample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
